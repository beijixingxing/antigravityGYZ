import { FastifyRequest, FastifyReply } from 'fastify';
import { PrismaClient, CredentialStatus } from '@prisma/client';
import { stream, request as undiciRequest } from 'undici';
import { CredentialPoolManager } from '../services/CredentialPoolManager';
import { PassThrough, Transform } from 'stream';
import { getUserAgent } from '../utils/system';
import { makeHttpError, isHttpError } from '../utils/http';
import { mergeSafetySettings, transformTools } from '../utils/gemini_transforms';
import { antigravityTokenManager } from '../services/AntigravityTokenManager';
import { AntigravityService } from '../services/AntigravityService';
import { isAntigravityModel, extractRealModelName, getAntigravityModelNames, ANTIGRAVITY_SUFFIX } from '../config/antigravityConfig';
import { mapModelName } from '../utils/antigravityUtils';
import { calculateCliQuotaLimits, resolveCliUsageGroup, getCliUsageCount, CliQuotaLimits } from '../utils/cliQuota';
import {
    detectRequestFormat,
    normalizeToOpenAI,
    openaiResponseToGemini,
    openaiResponseToAnthropic,
    openaiStreamChunkToGemini,
    openaiStreamChunkToAnthropic,
    RequestFormat
} from '../utils/formatConverter';
import { redis } from '../utils/redis';

const prisma = new PrismaClient();
const poolManager = new CredentialPoolManager();

// Lua è„šæœ¬ï¼šåŸå­åŒ– RPM é™åˆ¶æ£€æŸ¥
// è¿”å›å€¼: å½“å‰è®¡æ•° (å¦‚æœå·²è¶…é™è¿”å› -1)
const RATE_LIMIT_SCRIPT = `
local key = KEYS[1]
local limit = tonumber(ARGV[1])
local ttl = tonumber(ARGV[2])

local current = redis.call('INCR', key)
if current == 1 then
    redis.call('EXPIRE', key, ttl)
end

if current > limit then
    return -1
end
return current
`;

// ä½¿ç”¨ UTC+8 æ—¶åŒºè®¡ç®—ä»Šæ—¥æ—¥æœŸå­—ç¬¦ä¸²ï¼Œä¸ today_used é‡ç½®æ—¶é—´ä¸€è‡´
function getTodayStrUTC8(): string {
    const now = new Date();
    const utc8Offset = 8 * 60 * 60 * 1000;
    return new Date(now.getTime() + utc8Offset).toISOString().split('T')[0];
}

// --- Model Configuration (Ported from gcli2api/config.py) ---

const DEFAULT_SAFETY_SETTINGS = [
    { category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_NONE" },
    { category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_NONE" },
    { category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold: "BLOCK_NONE" },
    { category: "HARM_CATEGORY_DANGEROUS_CONTENT", threshold: "BLOCK_NONE" },
    { category: "HARM_CATEGORY_CIVIC_INTEGRITY", threshold: "BLOCK_NONE" },
];

function getAvailableModels() {
    // Cloud Code æ¸ é“æ¨¡å‹
    const cliSuffix = '-[æ˜Ÿæ˜Ÿå…¬ç›Šç«™-CLIæ¸ é“]';
    const withStreamVariants = (model: string) => [
        `${model}-çœŸæµ${cliSuffix}`,
        `${model}-å‡æµ${cliSuffix}`
    ];
    const cloudCodeModels = [
        ...withStreamVariants('gemini-2.5-flash'),
        ...withStreamVariants('gemini-2.5-pro'),
        // Disabled: gemini-3-pro-preview (CLI)
        // ...withStreamVariants('gemini-3-pro-preview'),
        ...withStreamVariants('gemini-3-pro-high'),
        ...withStreamVariants('gemini-3-pro-low'),
        ...withStreamVariants('gemini-3-flash-preview'),
        ...withStreamVariants('gemini-3-flash-minimal'),
        ...withStreamVariants('gemini-3-flash-medium'),
        ...withStreamVariants('gemini-3-flash-low'),
        ...withStreamVariants('gemini-3-flash-high')
    ];

    // åé‡åŠ›æ¸ é“æ¨¡å‹
    const antigravityModels = getAntigravityModelNames();

    return [...cloudCodeModels, ...antigravityModels];
}

function resolveCloudCodeModelName(modelName: string): string {
    const normalized = modelName.toLowerCase();
    if (normalized === 'gemini-3-pro-image-preview') return 'gemini-3-pro-image';
    if (normalized === 'gemini-3-pro-high' || normalized === 'gemini-3-pro-low') return 'gemini-3-pro-preview';
    if (normalized === 'gemini-3-flash-minimal'
        || normalized === 'gemini-3-flash-medium'
        || normalized === 'gemini-3-flash-low'
        || normalized === 'gemini-3-flash-high') {
        return 'gemini-3-flash-preview';
    }
    return modelName;
}

/**
 * ä»£ç†æ§åˆ¶å™¨
 * å¤„ç†å„ç§ AI æ¨¡å‹çš„è¯·æ±‚ä»£ç†å’Œè½¬æ¢
 * æ”¯æŒ OpenAIã€Geminiã€Anthropic ç­‰å¤šç§æ ¼å¼
 */
export class ProxyController {
    /** æ¨¡å‹ç¼“å­˜ï¼Œ5åˆ†é’Ÿè¿‡æœŸ */
    private static modelsCache: { data: any[]; expiresAt: number } | null = null;

    /**
     * å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚
     * æ”¯æŒå¤šç§æ¨¡å‹æ ¼å¼å’Œæ¸ é“
     */
    static async handleChatCompletion(req: FastifyRequest, reply: FastifyReply) {
        const authHeader = req.headers.authorization;
        if (!authHeader?.startsWith('Bearer ')) {
            return reply.code(401).send({ error: 'ç¼ºå°‘ API å¯†é’¥' });
        }
        const apiKeyStr = authHeader.replace('Bearer ', '').trim();

        // 0. å¿«é€Ÿå¥åº·æ£€æŸ¥ (åœ¨æ•°æ®åº“/è®¤è¯ä¹‹å‰)
        // ç«‹å³æ‹¦æˆª "Hi" æ¶ˆæ¯ä»¥åŠ é€Ÿè¿æ¥æµ‹è¯•
        try {
            const body = req.body as any;
            const messages = body.messages || [];
            if (messages.length === 1 && messages[0].role === 'user' && messages[0].content === 'Hi') {
                return reply.send({
                    choices: [{ message: { role: 'assistant', content: 'Gemini Proxy æ­£å¸¸å·¥ä½œä¸­' } }]
                });
            }
        } catch (e) { }

        // 1. è®¤è¯å’Œé€Ÿç‡é™åˆ¶
        const apiKeyData = await prisma.apiKey.findUnique({
            where: { key: apiKeyStr },
            include: { user: true }
        });

        if (!apiKeyData || !apiKeyData.is_active) {
            return reply.code(401).send({ error: 'æ— æ•ˆæˆ–å·²ç¦ç”¨çš„ API å¯†é’¥' });
        }

        const user = apiKeyData.user;

        if (!user.is_active) {
            return reply.code(401).send({ error: 'ğŸš« æ‚¨çš„è´¦æˆ·å·²è¢«ç¦ç”¨ï¼Œè¯·è”ç³»ç®¡ç†å‘˜è§£å°ã€‚' });
        }

        const isAdminKey = (apiKeyData as any).type === 'ADMIN';
        let cliQuotaLimits: CliQuotaLimits | null = null;

        // æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶è¦æ±‚ Discord ç»‘å®š
        const forceBindSetting = await prisma.systemSetting.findUnique({ where: { key: 'FORCE_DISCORD_BIND' } });
        const forceDiscordBind = forceBindSetting ? forceBindSetting.value === 'true' : false;
        if (forceDiscordBind && !isAdminKey) {
            const userFull = await prisma.user.findUnique({ where: { id: user.id } }) as any;
            if (!userFull?.discordId) {
                return reply.code(401).send({ error: 'è¯·å…ˆç»‘å®š Discord è´¦æˆ·åå†ä½¿ç”¨æœåŠ¡' });
            }
        }

        // è·å–å‡­è¯è®¡æ•°ï¼ˆç”¨äºæƒé™æ£€æŸ¥å’Œé…é¢è®¡ç®—ï¼‰
        // å†·å´çš„å‡­è¯ä»ç„¶ç®—å…¥é…é¢å¢é‡ï¼Œåªæœ‰ DEAD çš„ä¸ç®—
        const activeCredCount = await prisma.googleCredential.count({
            where: { owner_id: user.id, status: { in: [CredentialStatus.ACTIVE, CredentialStatus.COOLING] } }
        });
        const activeV3CredCount = await prisma.googleCredential.count({
            where: { owner_id: user.id, status: { in: [CredentialStatus.ACTIVE, CredentialStatus.COOLING] }, supports_v3: true }
        });

        // å…¨å±€å…±äº«æ¨¡å¼æ‹¦æˆªå·²ç§»é™¤ï¼›åˆ†åˆ«åœ¨å„æ¸ é“åˆ†æ”¯å†…è¿›è¡Œè®¿é—®æ§åˆ¶

        if (!isAdminKey) {
            // è·å–ç³»ç»Ÿé…ç½®
            const configSetting = await prisma.systemSetting.findUnique({ where: { key: 'SYSTEM_CONFIG' } });
            let rateLimit = 10; // é»˜è®¤èŒæ–°é€Ÿç‡é™åˆ¶

            /**
             * è¾…åŠ©å‡½æ•°ï¼šä»æ–°åµŒå¥—æ ¼å¼æˆ–æ—§æ•°å­—æ ¼å¼ä¸­æå–é…é¢å€¼
             * @param levelConfig ç­‰çº§é…ç½®
             * @param defaultValue é»˜è®¤å€¼
             * @returns åŒ…å«åŸºç¡€é…é¢å’Œå¢é‡é…é¢çš„å¯¹è±¡
             */
            const getQuotaValue = (levelConfig: any, defaultValue: number): { base: { flash: number; pro: number; v3: number }, increment: { flash: number; pro: number; v3: number } } => {
              if (typeof levelConfig === 'number') {
                // æ—§æ ¼å¼ï¼šå•ä¸€æ•°å­—ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…ç»™å„æ¨¡å‹
                return { base: { flash: levelConfig, pro: Math.floor(levelConfig / 4), v3: Math.floor(levelConfig / 4) }, increment: { flash: 0, pro: 0, v3: 0 } };
              }
              if (levelConfig && typeof levelConfig === 'object' && levelConfig.base) {
                // æ–°åµŒå¥—æ ¼å¼ï¼šåˆ†æ¨¡å‹é…ç½®åŸºç¡€é…é¢å’Œå¢é‡é…é¢
                return {
                  base: {
                    flash: levelConfig.base?.flash ?? defaultValue,
                    pro: levelConfig.base?.pro ?? Math.floor(defaultValue / 4),
                    v3: levelConfig.base?.v3 ?? Math.floor(defaultValue / 4)
                  },
                  increment: {
                    flash: levelConfig.increment?.flash ?? 0,
                    pro: levelConfig.increment?.pro ?? 0,
                    v3: levelConfig.increment?.v3 ?? 0
                  }
                };
              }
              // é»˜è®¤å€¼å¤„ç†
              return { base: { flash: defaultValue, pro: Math.floor(defaultValue / 4), v3: Math.floor(defaultValue / 4) }, increment: { flash: 0, pro: 0, v3: 0 } };
            };

            let totalQuota = 300; // é»˜è®¤é…é¢
            cliQuotaLimits = { flash: totalQuota, pro: totalQuota, v3: totalQuota, total: totalQuota };

            if (configSetting) {
                try {
                    const conf = JSON.parse(configSetting.value);
                    const limits = conf.rate_limit || {};

                    // é€Ÿç‡é™åˆ¶é€»è¾‘ï¼ˆåŸºäºç”¨æˆ·ç­‰çº§/V3å‡­è¯æ•°é‡ï¼‰
                    if (activeV3CredCount > 0) rateLimit = limits.v3_contributor ?? 120; // V3è´¡çŒ®è€…ï¼š120 RPM
                    else if (activeCredCount > 0) rateLimit = limits.contributor ?? 60; // è´¡çŒ®è€…ï¼š60 RPM
                    else rateLimit = limits.newbie ?? 10; // èŒæ–°ï¼š10 RPM

                    const quotaConf = conf.quota || {};
                    
                    // æ ¹æ®ç”¨æˆ·ç­‰çº§è·å–é…é¢é…ç½®
                    let levelQuota: { base: { flash: number; pro: number; v3: number }, increment: { flash: number; pro: number; v3: number } };
                    if (activeV3CredCount > 0) {
                        // V3è´¡çŒ®è€…ï¼š3000é»˜è®¤é…é¢
                        levelQuota = getQuotaValue(quotaConf.v3_contributor, 3000);
                    } else if (activeCredCount > 0) {
                        // è´¡çŒ®è€…ï¼š1500é»˜è®¤é…é¢
                        levelQuota = getQuotaValue(quotaConf.contributor, 1500);
                    } else {
                        // èŒæ–°ï¼š300é»˜è®¤é…é¢
                        levelQuota = getQuotaValue(quotaConf.newbie, 300);
                    }
                    
                    // è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„æ€»é…é¢ï¼ˆåŸºç¡€é…é¢ + é¢å¤–å‡­è¯å¢é‡ï¼‰
                    const additionalCreds = Math.max(0, activeCredCount - 1); // å‡å»ç¬¬ä¸€ä¸ªå‡­è¯ï¼Œåªè®¡ç®—é¢å¤–å‡­è¯
                    const flashQuota = levelQuota.base.flash + additionalCreds * levelQuota.increment.flash;
                    const proQuota = levelQuota.base.pro + additionalCreds * levelQuota.increment.pro;
                    const v3ModelQuota = levelQuota.base.v3 + additionalCreds * levelQuota.increment.v3;
                    
                    // æ—§ç‰ˆå¢é‡å…¼å®¹ï¼šæ”¯æŒæ—§çš„ increment_per_credential å­—æ®µ
                    const legacyInc = quotaConf.increment_per_credential ?? 0;
                    const legacyExtra = additionalCreds * legacyInc;
                    
                    // æ€»é…é¢ï¼šæ‰€æœ‰æ¨¡å‹é…é¢ä¹‹å’Œ + æ—§ç‰ˆå¢é‡
                    totalQuota = flashQuota + proQuota + v3ModelQuota + legacyExtra;
                    cliQuotaLimits = calculateCliQuotaLimits(conf, activeCredCount, activeV3CredCount);
                } catch (e) {
                    console.error('è§£æç³»ç»Ÿé…ç½®å¤±è´¥:', e);
                }
            }

            // é…é¢æ£€æŸ¥ï¼šæ£€æŸ¥ç”¨æˆ·ä»Šæ—¥ä½¿ç”¨é‡æ˜¯å¦è¶…è¿‡æ€»é…é¢
            if (user.today_used >= totalQuota) {
                return reply.code(402).send({ error: `æ¯æ—¥é…é¢å·²ç”¨å®Œ (${user.today_used}/${totalQuota})` });
            }

            // é€Ÿç‡é™åˆ¶æ£€æŸ¥ï¼šä½¿ç”¨ Redis å®ç°æ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶
            const rateKey = `RATE_LIMIT:${user.id}`;
            const currentRate = await redis.incr(rateKey);
            if (currentRate === 1) {
                await redis.expire(rateKey, 60); // è®¾ç½® 60 ç§’è¿‡æœŸ
            }
            if (currentRate > rateLimit) {
                return reply.code(429).send({ error: `é€Ÿç‡é™åˆ¶å·²è¶…å‡º (${rateLimit}/åˆ†é’Ÿ)` });
            }
        }

        // 2. è§£æè¯·æ±‚
        const openAIBody = req.body as any;
        // å…¼å®¹ prompt å‚æ•°æ ¼å¼ï¼šå¦‚æœæ²¡æœ‰ messagesï¼Œå°† prompt è½¬æ¢ä¸º messages
        if (!openAIBody.messages && typeof openAIBody.prompt === 'string') {
            openAIBody.messages = [{ role: 'user', content: String(openAIBody.prompt) }];
        }
        // å°† temperature é™åˆ¶åœ¨åˆç†èŒƒå›´å†…ï¼Œé¿å…æç«¯å€¼å½±å“åé‡åŠ›æ¸ é“
        if (typeof openAIBody.temperature === 'number') {
            openAIBody.temperature = Math.min(1.0, Math.max(0.1, openAIBody.temperature));
        }
        const requestedModel = openAIBody.model;
        const isStreaming = openAIBody.stream === true;

        // æ£€æŸ¥æ˜¯å¦æ˜¯åé‡åŠ›æ¸ é“æ¨¡å‹
        if (isAntigravityModel(requestedModel)) {
            return ProxyController.handleAntigravityRequest(req, reply, openAIBody, user, isAdminKey);
        }

        // Cloud Code æ¸ é“æ¨¡å‹æ˜ å°„é€»è¾‘
        let realModelName = requestedModel;
        let useFakeStream = false;

        // å¤„ç†æ¨¡å‹åç¼€ï¼šç§»é™¤å…¬ç›Šç«™ç›¸å…³åç¼€
        if (requestedModel.includes('-[æ˜Ÿæ˜Ÿå…¬ç›Šç«™-CLIæ¸ é“]') || requestedModel.includes('-[æ˜Ÿæ˜Ÿå…¬ç›Šç«™-ä»»ä½•æ”¶è´¹éƒ½æ˜¯éª—å­]') || requestedModel.includes('-[æ˜Ÿæ˜Ÿå…¬ç›Šç«™-æ‰€æœ‰æ”¶è´¹éƒ½éª—å­]')) {
            // ç§»é™¤åç¼€
            let base = requestedModel.replace('-[æ˜Ÿæ˜Ÿå…¬ç›Šç«™-CLIæ¸ é“]', '').replace('-[æ˜Ÿæ˜Ÿå…¬ç›Šç«™-ä»»ä½•æ”¶è´¹éƒ½æ˜¯éª—å­]', '').replace('-[æ˜Ÿæ˜Ÿå…¬ç›Šç«™-æ‰€æœ‰æ”¶è´¹éƒ½éª—å­]', '');

            // æ£€æŸ¥æµç­–ç•¥
            if (base.includes('-å‡æµ')) {
                useFakeStream = true;
                realModelName = base.replace('-å‡æµ', '');
            } else if (base.includes('-çœŸæµ')) {
                realModelName = base.replace('-çœŸæµ', '');
            } else {
                realModelName = base;
            }
        }

        // V3 æ¨¡å‹é€»è¾‘
        const isV3Model = realModelName.includes('gemini-3') || realModelName.includes('gemini-exp');
        let poolType: 'GLOBAL' | 'V3' = 'GLOBAL';

        if (isV3Model) {
            // æ£€æŸ¥ V3 æƒé™
            const isAdmin = user.role === 'ADMIN';
            const hasV3Creds = activeV3CredCount > 0;
            // æ–°å¢å¼€å…³ï¼šå…è®¸æœªä¸Šä¼ æˆ–æ— 3.0Proæƒé™ä¹Ÿå¯ä½¿ç”¨3.0ç³»åˆ—ï¼ˆCLIï¼‰
            const openAccessSetting = await prisma.systemSetting.findUnique({ where: { key: 'ENABLE_GEMINI3_OPEN_ACCESS' } });
            const enableOpenAccess = openAccessSetting ? openAccessSetting.value === 'true' : false;
            if (!enableOpenAccess) {
                if (!isAdmin && !hasV3Creds && !isAdminKey) {
                    return reply.code(401).send({
                        error: 'ğŸ”’ æ­¤æ¨¡å‹ (Gemini 3.0) ä»…é™ç®¡ç†å‘˜æˆ–ä¸Šä¼ äº† 3.0 å‡­è¯çš„ç”¨æˆ·ä½¿ç”¨ã€‚è¯·å…ˆè´¡çŒ® 3.0 å‡­è¯ï¼'
                    });
                }
            }
            poolType = 'V3';
        }

        // CLI å…±äº«æ¨¡å¼ï¼ˆä»… Cloud Code æ¸ é“ï¼‰
        const cliSharedSetting = await prisma.systemSetting.findUnique({ where: { key: 'ENABLE_CLI_SHARED_MODE' } });
        let isCliSharedMode = cliSharedSetting ? cliSharedSetting.value === 'true' : true;
        if (cliSharedSetting == null) {
            // å…¼å®¹æ—§é”®
            const legacy = await prisma.systemSetting.findUnique({ where: { key: 'ENABLE_SHARED_MODE' } });
            isCliSharedMode = legacy ? legacy.value === 'true' : true;
        }
        if (!isCliSharedMode && !isAdminKey) {
            const isAdmin = user.role === 'ADMIN';
            const hasCliCredential = activeCredCount > 0;
            if (!isAdmin && !hasCliCredential) {
                return reply.code(401).send({
                    error: 'ğŸ”’ å·²å…³é—­ CLI å…±äº«æ¨¡å¼ï¼šä»…ä¸Šä¼ è¿‡ CLI å‡­è¯çš„ç”¨æˆ·å¯ä»¥ä½¿ç”¨ Cloud Code æ¸ é“ã€‚'
                });
            }
        }

        if (!isAdminKey && cliQuotaLimits) {
            const { group } = resolveCliUsageGroup(realModelName);
            if (group !== 'other') {
                try {
                    const used = await getCliUsageCount(user.id, group);
                    const limit = cliQuotaLimits[group];
                    if (Number.isFinite(limit) && used >= limit) {
                        return reply.code(402).send({ error: `æ¨¡å‹ ${realModelName} æ¯æ—¥é…é¢å·²ç”¨å®Œ (${used}/${limit})` });
                    }
                } catch (e: any) {
                    console.error('[Proxy] æ¨¡å‹é…é¢æ£€æŸ¥å¤±è´¥:', e?.message || e);
                }
            }
        }

        try {
            // 3. Transform Request 
            const modifiedBody = { ...openAIBody, model: realModelName };
            const geminiPayload = ProxyController.transformOpenAIToGemini(modifiedBody);

            // 4. Execute
            if (isStreaming) {
                if (useFakeStream) {
                    await ProxyController.handleFakeStreamRequest(req, reply, realModelName, geminiPayload, user, isAdminKey, poolType);
                } else {
                    await ProxyController.handleStreamRequest(req, reply, realModelName, geminiPayload, user, isAdminKey, poolType);
                }
            } else {
                await ProxyController.handleStandardRequest(req, reply, realModelName, geminiPayload, user, isAdminKey, poolType);
            }

        } catch (err: any) {
            console.error('[Proxy] Error:', err);
            const errPayload = { error: { message: err.message || 'Internal Server Error', type: 'server_error' } };
            if (!reply.raw.headersSent) {
                reply.code(500).send(errPayload);
            }
        }
    }

    static async handleListModels(req: FastifyRequest, reply: FastifyReply) {
        const now = Date.now();
        if (ProxyController.modelsCache && ProxyController.modelsCache.expiresAt > now) {
            return reply.send({ object: 'list', data: ProxyController.modelsCache.data });
        }
        const models = getAvailableModels();

        const data = models.map(id => ({
            id,
            object: 'model',
            created: Math.floor(Date.now() / 1000), // Dynamic created time
            owned_by: 'google',
            permission: [],
            root: id,
            parent: null,
        }));

        ProxyController.modelsCache = { data, expiresAt: now + 5 * 60 * 1000 };
        return reply.send({ object: 'list', data });
    }

    /**
     * Gemini æ ¼å¼çš„æ¨¡å‹åˆ—è¡¨
     * è·¯ç”±: GET /v1/models (å½“æ£€æµ‹åˆ° Gemini å®¢æˆ·ç«¯æ—¶ä½¿ç”¨)
     */
    static async handleListModelsGemini(req: FastifyRequest, reply: FastifyReply) {
        const models = getAvailableModels();

        const geminiModels = models.map(id => ({
            name: `models/${id}`,
            version: '001',
            displayName: id,
            description: `Model ${id}`,
            inputTokenLimit: 1000000,
            outputTokenLimit: 65536,
            supportedGenerationMethods: ['generateContent', 'streamGenerateContent'],
            temperature: 1.0,
            topP: 0.95,
            topK: 64
        }));

        return reply.send({ models: geminiModels });
    }

    /**
     * Anthropic æ ¼å¼çš„æ¨¡å‹åˆ—è¡¨ï¼ˆæ¨¡æ‹Ÿç«¯ç‚¹ï¼‰
     * è·¯ç”±: GET /v1/models (å½“æ£€æµ‹åˆ° Anthropic å®¢æˆ·ç«¯æ—¶ä½¿ç”¨)
     */
    static async handleListModelsAnthropic(req: FastifyRequest, reply: FastifyReply) {
        const models = getAvailableModels();

        // åªè¿”å› Claude ç›¸å…³æ¨¡å‹
        const claudeModels = models.filter(id => id.includes('claude'));

        const anthropicModels = claudeModels.map(id => ({
            type: 'model',
            id: id,
            display_name: id,
            created_at: new Date().toISOString()
        }));

        // Anthropic æ²¡æœ‰å®˜æ–¹æ¨¡å‹åˆ—è¡¨ç«¯ç‚¹ï¼Œè¿™æ˜¯æ¨¡æ‹Ÿæ ¼å¼
        return reply.send({
            data: anthropicModels,
            has_more: false,
            first_id: anthropicModels[0]?.id || null,
            last_id: anthropicModels[anthropicModels.length - 1]?.id || null
        });
    }

    // --- Antigravity æ¸ é“å¤„ç† ---

    private static async handleAntigravityRequest(
        req: FastifyRequest,
        reply: FastifyReply,
        openAIBody: any,
        user: any,
        isAdminKey: boolean
    ) {
        const requestedModel = openAIBody.model;
        const isStreaming = openAIBody.stream === true;

        const realModel = extractRealModelName(requestedModel);
        const actualModelId = mapModelName(realModel);
        const group = realModel.includes('gemini-3') ? 'gemini3' : 'claude';

        // Load Antigravity Config
        let claudeLimit = 100;
        let gemini3Limit = 200;
        let useTokenQuota = false;
        let claudeTokenQuota = 100000;
        let gemini3TokenQuota = 200000;
        let agRateLimit = 30; // åé‡åŠ›æ¸ é“æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶
        let agRateLimitIncrement = 0;
        let poolRoundRobin = true;
        let config: any = {};

        try {
            const configSetting = await prisma.systemSetting.findUnique({ where: { key: 'ANTIGRAVITY_CONFIG' } });
            if (configSetting) {
                config = JSON.parse(configSetting.value);
                claudeLimit = config.claude_limit ?? 100;
                gemini3Limit = config.gemini3_limit ?? 200;
                useTokenQuota = !!config.use_token_quota;
                claudeTokenQuota = config.claude_token_quota ?? 100000;
                gemini3TokenQuota = config.gemini3_token_quota ?? 200000;
                agRateLimit = config.rate_limit ?? 30;
                agRateLimitIncrement = config.rate_limit_increment ?? 0;
                poolRoundRobin = config.pool_round_robin ?? true;
            }
        } catch (e) {
            console.error('Failed to load ANTIGRAVITY_CONFIG', e);
        }

        // Calculate Increment based on User's Tokens (ACTIVE + COOLING)
        // å†·å´çš„å‡­è¯ä»ç„¶ç®—å…¥é…é¢å¢é‡ï¼Œåªæœ‰ DEAD çš„ä¸ç®—
        const userTokenCount = await prisma.antigravityToken.count({
            where: {
                owner_id: user.id,
                status: { in: ['ACTIVE', 'COOLING'] },
                is_enabled: true
            }
        });

        const effectiveAgRateLimit = agRateLimit + (userTokenCount > 0 ? agRateLimitIncrement : 0);

        // åé‡åŠ›æ¸ é“é€Ÿç‡é™åˆ¶æ£€æŸ¥ï¼ˆæ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶ï¼‰- ä½¿ç”¨ Lua è„šæœ¬ä¿è¯åŸå­æ€§
        if (!isAdminKey && effectiveAgRateLimit > 0) {
            const now = Math.floor(Date.now() / 60000); // å½“å‰åˆ†é’Ÿ
            const rateKey = `AG_RATE:${user.id}:${now}`;
            
            // ä½¿ç”¨ Lua è„šæœ¬åŸå­åŒ–æ‰§è¡Œ: INCR + æ¡ä»¶ EXPIRE + é™åˆ¶æ£€æŸ¥
            // è¿”å› -1 è¡¨ç¤ºå·²è¶…é™ï¼Œå¦åˆ™è¿”å›å½“å‰è®¡æ•°
            const result = await redis.eval(RATE_LIMIT_SCRIPT, 1, rateKey, effectiveAgRateLimit, 120) as number;

            if (result === -1) {
                return reply.code(429).send({
                    error: {
                        message: `åé‡åŠ›æ¸ é“é€Ÿç‡é™åˆ¶ï¼šæ¯åˆ†é’Ÿæœ€å¤š ${effectiveAgRateLimit} æ¬¡è¯·æ±‚ï¼Œè¯·ç¨åå†è¯•`,
                        type: 'rate_limit_exceeded'
                    }
                });
            }
        }

        // Calculate Base Limit (Token or Request count)
        const base = useTokenQuota
            ? (group === 'gemini3' ? gemini3TokenQuota : claudeTokenQuota)
            : (group === 'gemini3' ? gemini3Limit : claudeLimit);

        const inc = useTokenQuota
            ? (group === 'gemini3' ? config.increment_token_per_token_gemini3 : config.increment_token_per_token_claude)
            : (group === 'gemini3' ? config.increment_per_token_gemini3 : config.increment_per_token_claude);

        const computedLimit = base + (userTokenCount > 0 ? userTokenCount * (inc || 0) : 0);

        const todayStr = getTodayStrUTC8();

        // Dual Keys: Always maintain both counters
        const usageKeyRequests = `USAGE:requests:${todayStr}:${user.id}:antigravity:${group}`;
        const usageKeyTokens = `USAGE:tokens:${todayStr}:${user.id}:antigravity:${group}`;

        // Legacy Key (Fallback/Migration) - eventually we can deprecate this
        // But for now, let's just use the specific keys for logic

        const strictSetting = await prisma.systemSetting.findUnique({ where: { key: 'ANTIGRAVITY_STRICT_MODE' } });
        const strictMode = strictSetting ? strictSetting.value === 'true' : false;

        // ä¸¥æ ¼æ¨¡å¼ï¼šåªæœ‰ä¸Šä¼ è¿‡åé‡åŠ›å‡­è¯çš„ç”¨æˆ·æ‰èƒ½ä½¿ç”¨åé‡åŠ›æ¸ é“
        if (!isAdminKey && user.role !== 'ADMIN' && strictMode) {
            const hasAccess = await antigravityTokenManager.hasAntigravityAccess(user.id);
            if (!hasAccess) {
                console.warn('[Antigravity] Strict mode enabled, user without valid credential blocked:', user.id);
                return reply.code(401).send({
                    error: {
                        message: 'ğŸ”’ å·²å¼€å¯åé‡åŠ›ä¸¥æ ¼æ¨¡å¼ï¼šä»…ä¸Šä¼ è¿‡æœ‰æ•ˆå‡­è¯çš„ç”¨æˆ·å¯ä»¥ä½¿ç”¨åé‡åŠ›æ¸ é“ã€‚',
                        type: 'forbidden'
                    }
                });
            }
        }

        // é…é¢æ£€æŸ¥ï¼ˆæ— è®ºä¸¥æ ¼æ¨¡å¼æ˜¯å¦å¼€å¯éƒ½æ£€æŸ¥ï¼‰
        if (!isAdminKey) {
            const userOverride = group === 'gemini3' ? user.ag_gemini3_limit : user.ag_claude_limit;
            const effectiveLimit = (userOverride && userOverride > 0) ? userOverride : computedLimit;

            // Check limit based on current mode
            const current = parseInt((await redis.get(useTokenQuota ? usageKeyTokens : usageKeyRequests)) || '0', 10);

            if (current >= effectiveLimit) {
                const unit = useTokenQuota ? 'Tokens' : 'Requests';
                return reply.code(402).send({
                    error: { message: `Antigravity ${group} daily limit reached (${current}/${effectiveLimit} ${unit})`, type: 'quota_exceeded' }
                });
            }
        }

        // è·å– Antigravity Token (ä»å…¬å…±æ± ï¼ŒæŒ‰ç”¨æˆ·é”å®šé¿å…è·¨ç”¨æˆ·å¹¶å‘å…±äº«)
        const initialTtl = isStreaming ? 60000 : 30000;
        const token = await antigravityTokenManager.getToken({ group: group as 'claude' | 'gemini3', modelId: actualModelId, poolRoundRobin }, user.id, initialTtl);
        if (!token) {
            return reply.code(503).send({
                error: { message: 'æ²¡æœ‰å¯ç”¨çš„åé‡åŠ›æ¸ é“ Tokenï¼Œè¯·è”ç³»ç®¡ç†å‘˜æ·»åŠ ', type: 'service_unavailable' }
            });
        }

        console.log(`[Antigravity] å¤„ç†è¯·æ±‚: ${requestedModel} -> ${realModel}, streaming: ${isStreaming}`);

        const responseId = 'chatcmpl-' + crypto.randomUUID();
        const created = Math.floor(Date.now() / 1000);

        try {
            if (isStreaming) {
                // æµå¼å“åº”
                reply.raw.writeHead(200, {
                    'Content-Type': 'text/event-stream',
                    'Cache-Control': 'no-cache',
                    'Connection': 'keep-alive',
                    'X-Accel-Buffering': 'no'
                });

                let tokenUsed = false;
                let usageTokens = 0;

                // ç«‹å³è®¡æ•°è¯·æ±‚æ¬¡æ•°ï¼ˆä¸ç­‰å¾… usage äº‹ä»¶ï¼‰
                try {
                    await redis.incr(usageKeyRequests);
                    const now = new Date();
                    const tomorrow = new Date(now.getFullYear(), now.getMonth(), now.getDate() + 1);
                    const seconds = Math.floor((tomorrow.getTime() - now.getTime()) / 1000);
                    await redis.expire(usageKeyRequests, seconds);
                    await redis.hincrby(`AG_GLOBAL:requests:${todayStr}`, group, 1);
                    await redis.expire(`AG_GLOBAL:requests:${todayStr}`, 86400);
                    console.log(`[Antigravity] è¯·æ±‚è®¡æ•°æˆåŠŸ: ${usageKeyRequests}`);
                } catch (e) {
                    console.error('[Antigravity] è¯·æ±‚è®¡æ•°å¤±è´¥:', e);
                }

                let attempts = 0;
                let currentToken = token;
                const onData = async (data: any) => {
                    // è®°å½• Token ä½¿ç”¨
                    if (!tokenUsed) {
                        await prisma.antigravityToken.update({
                            where: { id: currentToken.id },
                            data: { total_used: { increment: 1 }, last_used_at: new Date(), fail_count: 0 }
                        }).catch(() => { });
                        tokenUsed = true;
                    }
                    // æ”¶é›† Token ç”¨é‡ï¼ˆè¯·æ±‚æ¬¡æ•°å·²åœ¨æµå¼€å§‹å‰è®¡æ•°ï¼‰
                    if (data.type === 'usage') {
                        usageTokens = data.usage?.total_tokens || 0;
                    }

                    if (data.type === 'text') {
                        const chunk = {
                            id: responseId,
                            object: 'chat.completion.chunk',
                            created,
                            model: requestedModel,
                            choices: [{
                                index: 0,
                                delta: { content: data.content },
                                finish_reason: null
                            }]
                        };
                        reply.raw.write(`data: ${JSON.stringify(chunk)}\n\n`);
                    } else if (data.type === 'reasoning') {
                        // æ€ç»´å†…å®¹ -> reasoning_content å­—æ®µï¼ˆå’Œ CLI ä¸€è‡´ï¼‰
                        const chunk = {
                            id: responseId,
                            object: 'chat.completion.chunk',
                            created,
                            model: requestedModel,
                            choices: [{
                                index: 0,
                                delta: { reasoning_content: data.content },
                                finish_reason: null
                            }]
                        };
                        reply.raw.write(`data: ${JSON.stringify(chunk)}\n\n`);
                    } else if (data.type === 'tool_calls') {
                        const chunk = {
                            id: responseId,
                            object: 'chat.completion.chunk',
                            created,
                            model: requestedModel,
                            choices: [{
                                index: 0,
                                delta: { tool_calls: data.tool_calls },
                                finish_reason: null
                            }]
                        };
                        reply.raw.write(`data: ${JSON.stringify(chunk)}\n\n`);
                    } else if (data.type === 'usage') {
                        // å‘é€ç»“æŸ chunk
                        const endChunk = {
                            id: responseId,
                            object: 'chat.completion.chunk',
                            created,
                            model: requestedModel,
                            choices: [{ index: 0, delta: {}, finish_reason: 'stop' }],
                            usage: data.usage
                        };
                        reply.raw.write(`data: ${JSON.stringify(endChunk)}\n\n`);
                    }
                };
                while (attempts < 5) {
                    try {
                        await AntigravityService.generateStreamResponse(
                            openAIBody.messages,
                            realModel,
                            openAIBody,
                            openAIBody.tools,
                            currentToken,
                            onData
                        );
                        break;
                    } catch (err: any) {
                        const status = err?.statusCode || err?.response?.status;
                        const msg = err?.body || err?.message || '';
                        if (status === 429 || /Resource has been exhausted/i.test(String(msg))) {
                            let cooldownMs = 60000;
                            try {
                                const obj = JSON.parse(String(msg));
                                const details = obj?.error?.details || [];
                                for (const d of details) {
                                    if (d['@type'] && String(d['@type']).includes('google.rpc.ErrorInfo')) {
                                        const ts = d?.metadata?.quotaResetTimeStamp;
                                        const retryDelay = d?.metadata?.retryDelay ? parseInt(d.metadata.retryDelay, 10) : 0;
                                        if (ts) {
                                            const ms = new Date(ts).getTime() - Date.now();
                                            if (ms > 0) cooldownMs = ms;
                                        } else if (retryDelay > 0) {
                                            cooldownMs = retryDelay * 1000;
                                        }
                                        break;
                                    }
                                }
                            } catch { }
                            try { await antigravityTokenManager.markAsCooling(currentToken.id, cooldownMs); } catch { }
                            await antigravityTokenManager.releaseLock(currentToken.id, user.id);
                            const next = await antigravityTokenManager.getToken({ group: group as 'claude' | 'gemini3', poolRoundRobin }, user.id, 60000);
                            if (!next) throw err;
                            currentToken = next;
                            attempts++;
                            continue;
                        } else if (status === 403) {
                            try { await antigravityTokenManager.markAsDead(currentToken.id); } catch { }
                            await antigravityTokenManager.releaseLock(currentToken.id, user.id);
                            const next = await antigravityTokenManager.getToken({ group: group as 'claude' | 'gemini3', poolRoundRobin }, user.id, 60000);
                            if (!next) throw err;
                            currentToken = next;
                            attempts++;
                            continue;
                        } else if (status === 500) {
                            await antigravityTokenManager.releaseLock(currentToken.id, user.id);
                            const next = await antigravityTokenManager.getToken({ group: group as 'claude' | 'gemini3', poolRoundRobin }, user.id, 60000);
                            if (!next) throw err;
                            currentToken = next;
                            attempts++;
                            continue;
                        }
                        throw err;
                    }
                }

                // æµç»“æŸåæ›´æ–° Token ç”¨é‡
                // å¦‚æœæ²¡æœ‰æ”¶åˆ° usage äº‹ä»¶ï¼Œä½¿ç”¨ä¿åº•ä¼°ç®—å€¼
                const finalTokens = usageTokens > 0 ? usageTokens : 1000;
                console.log(`[Antigravity] æµå¼è¯·æ±‚ç»“æŸ, usageTokens=${usageTokens}, finalTokens=${finalTokens}`);
                try {
                    await redis.incrby(usageKeyTokens, finalTokens);
                    const now = new Date();
                    const tomorrow = new Date(now.getFullYear(), now.getMonth(), now.getDate() + 1);
                    const seconds = Math.floor((tomorrow.getTime() - now.getTime()) / 1000);
                    await redis.expire(usageKeyTokens, seconds);
                    await redis.hincrby(`AG_GLOBAL:tokens:${todayStr}`, group, finalTokens);
                    await redis.expire(`AG_GLOBAL:tokens:${todayStr}`, 86400);
                } catch (e) {
                    console.error('[Antigravity] Token è®¡æ•°å¤±è´¥:', e);
                }

                reply.raw.write('data: [DONE]\n\n');
                reply.raw.end();
                try { await antigravityTokenManager.releaseLock(currentToken.id, user.id); } catch { }

            } else {
                // éæµå¼å“åº” - ç«‹å³è®¡æ•°è¯·æ±‚æ¬¡æ•°
                try {
                    await redis.incr(usageKeyRequests);
                    const now = new Date();
                    const tomorrow = new Date(now.getFullYear(), now.getMonth(), now.getDate() + 1);
                    const seconds = Math.floor((tomorrow.getTime() - now.getTime()) / 1000);
                    await redis.expire(usageKeyRequests, seconds);
                    await redis.hincrby(`AG_GLOBAL:requests:${todayStr}`, group, 1);
                    await redis.expire(`AG_GLOBAL:requests:${todayStr}`, 86400);
                    console.log(`[Antigravity] éæµå¼è¯·æ±‚è®¡æ•°æˆåŠŸ: ${usageKeyRequests}`);
                } catch (e) {
                    console.error('[Antigravity] éæµå¼è¯·æ±‚è®¡æ•°å¤±è´¥:', e);
                }

                let attempts2 = 0;
                let currentToken2 = token;
                let gotResult = false, content = '', reasoningContent: string | undefined = undefined, toolCalls: any[] = [], usage: any = undefined;
                while (attempts2 < 5) {
                    try {
                        const res = await AntigravityService.generateResponse(
                            openAIBody.messages,
                            realModel,
                            openAIBody,
                            openAIBody.tools,
                            currentToken2,
                            { retry_on_429: true, max_retries: 5 }
                        );
                        content = res.content; reasoningContent = res.reasoningContent; toolCalls = res.toolCalls || []; usage = res.usage; gotResult = true;
                        break;
                    } catch (err: any) {
                        const status = err?.statusCode || err?.response?.status;
                        const msg = err?.body || err?.message || '';
                        if (status === 429 || /Resource has been exhausted/i.test(String(msg))) {
                            let cooldownMs = 60000;
                            try {
                                const obj = JSON.parse(String(msg));
                                const details = obj?.error?.details || [];
                                for (const d of details) {
                                    if (d['@type'] && String(d['@type']).includes('google.rpc.ErrorInfo')) {
                                        const ts = d?.metadata?.quotaResetTimeStamp;
                                        const retryDelay = d?.metadata?.retryDelay ? parseInt(d.metadata.retryDelay, 10) : 0;
                                        if (ts) {
                                            const ms = new Date(ts).getTime() - Date.now();
                                            if (ms > 0) cooldownMs = ms;
                                        } else if (retryDelay > 0) {
                                            cooldownMs = retryDelay * 1000;
                                        }
                                        break;
                                    }
                                }
                            } catch { }
                            try { await antigravityTokenManager.markAsCooling(currentToken2.id, cooldownMs); } catch { }
                            await antigravityTokenManager.releaseLock(currentToken2.id, user.id);
                            const next = await antigravityTokenManager.getToken({ group: group as 'claude' | 'gemini3', poolRoundRobin }, user.id, 30000);
                            if (!next) throw err;
                            currentToken2 = next;
                            attempts2++;
                            continue;
                        } else if (status === 403) {
                            try { await antigravityTokenManager.markAsDead(currentToken2.id); } catch { }
                            await antigravityTokenManager.releaseLock(currentToken2.id, user.id);
                            const next = await antigravityTokenManager.getToken({ group: group as 'claude' | 'gemini3', poolRoundRobin }, user.id, 30000);
                            if (!next) throw err;
                            currentToken2 = next;
                            attempts2++;
                            continue;
                        } else if (status === 500) {
                            await antigravityTokenManager.releaseLock(currentToken2.id, user.id);
                            const next = await antigravityTokenManager.getToken({ group: group as 'claude' | 'gemini3', poolRoundRobin }, user.id, 30000);
                            if (!next) throw err;
                            currentToken2 = next;
                            attempts2++;
                            continue;
                        }
                        throw err;
                    }
                }

                if (!gotResult) { throw makeHttpError(500, 'Failed to generate response after retries'); }

                // è®°å½• Token ä½¿ç”¨
                await prisma.antigravityToken.update({
                    where: { id: token.id },
                    data: { total_used: { increment: 1 }, last_used_at: new Date(), fail_count: 0 }
                }).catch(() => { });

                // æ›´æ–° Token ç”¨é‡ï¼ˆä½¿ç”¨ä¿åº•å€¼å¦‚æœæ²¡æœ‰ usageï¼‰
                const usageTokens = usage?.total_tokens || 1000;
                try {
                    await redis.incrby(usageKeyTokens, usageTokens);
                    const now = new Date();
                    const tomorrow = new Date(now.getFullYear(), now.getMonth(), now.getDate() + 1);
                    const seconds = Math.floor((tomorrow.getTime() - now.getTime()) / 1000);
                    await redis.expire(usageKeyTokens, seconds);
                    await redis.hincrby(`AG_GLOBAL:tokens:${todayStr}`, group, usageTokens);
                    await redis.expire(`AG_GLOBAL:tokens:${todayStr}`, 86400);
                } catch { }

                const message: any = { role: 'assistant', content };
                if (reasoningContent) {
                    message.reasoning_content = reasoningContent;
                }
                if (toolCalls.length > 0) {
                    message.tool_calls = toolCalls;
                }

                const responseObj = {
                    id: responseId,
                    object: 'chat.completion',
                    created,
                    model: requestedModel,
                    choices: [{
                        index: 0,
                        message,
                        finish_reason: toolCalls.length > 0 ? 'tool_calls' : 'stop'
                    }],
                    usage
                };
                try { await antigravityTokenManager.releaseLock(currentToken2.id, user.id); } catch { }
                return reply.send(responseObj);
            }

        } catch (error: any) {
            console.error('[Antigravity] è¯·æ±‚å¤±è´¥:', error.message);

            // å¤„ç† 429 é”™è¯¯
            if (isHttpError(error) && error.statusCode === 429) {
                const currentToken = await prisma.antigravityToken.findUnique({ where: { id: token.id } });
                const newFailCount = (currentToken?.fail_count || 0) + 1;

                if (newFailCount >= 3) {
                    // è¿ç»­ 3 æ¬¡ 429ï¼Œè¿›å…¥é•¿æœŸå†·å´ (3å°æ—¶)
                    await antigravityTokenManager.markAsCooling(token.id, 3 * 60 * 60 * 1000);
                    // é‡ç½®è®¡æ•°
                    await prisma.antigravityToken.update({
                        where: { id: token.id },
                        data: { fail_count: 0 }
                    });
                    console.log(`[Antigravity] Token #${token.id} è¿ç»­ 3 æ¬¡ 429ï¼Œè¿›å…¥å†·å´ 3 å°æ—¶`);
                } else {
                    // å•æ¬¡ 429ï¼Œç«‹å³è¿›å…¥çŸ­æœŸå†·å´ (5åˆ†é’Ÿ)
                    await antigravityTokenManager.markAsCooling(token.id, 5 * 60 * 1000);
                    // å¢åŠ è®¡æ•°
                    await prisma.antigravityToken.update({
                        where: { id: token.id },
                        data: { fail_count: newFailCount }
                    });
                    console.log(`[Antigravity] Token #${token.id} 429 æ¬¡æ•°: ${newFailCount}/3ï¼ŒçŸ­æœŸå†·å´ 5 åˆ†é’Ÿ`);
                }
            }
            if (isHttpError(error) && error.statusCode === 403) {
                await antigravityTokenManager.markAsDead(token.id);
            }

            const status = isHttpError(error) ? error.statusCode : 500;
            const type = status === 403 ? 'permission_denied' : (status === 404 ? 'not_found' : 'api_error');
            let outMsg = isHttpError(error) ? (error.body || error.message) : (error.message || 'Antigravity request failed');
            try {
                const parsed = JSON.parse(outMsg);
                outMsg = parsed?.error?.message || outMsg;
            } catch { }

            if (!reply.raw.headersSent) {
                return reply.code(status).send({
                    error: { message: outMsg, type, code: status }
                });
            } else {
                const errChunk = {
                    id: responseId,
                    object: 'chat.completion.chunk',
                    created,
                    model: requestedModel,
                    choices: [{
                        index: 0,
                        delta: { content: `\n\n[${type}: ${outMsg}]` },
                        finish_reason: 'stop'
                    }]
                };
                reply.raw.write(`data: ${JSON.stringify(errChunk)}\n\n`);
                reply.raw.write('data: [DONE]\n\n');
                reply.raw.end();
            }
            try {
                if (isStreaming) {
                    // currentToken may be rotated; ensure last lock released
                    // no-op if not held by this user
                    await antigravityTokenManager.releaseLock((token as any).id, user.id);
                } else {
                    await antigravityTokenManager.releaseLock((token as any).id, user.id);
                }
            } catch { }
        }
    }

    // --- Transformation Logic (Ported from openai_transfer.py) ---

    private static transformOpenAIToGemini(openaiRequest: any) {
        const contents: any[] = [];
        let systemInstructions: string[] = [];
        let tools: any[] = [];

        // 1. Messages Processing
        for (const msg of openaiRequest.messages) {
            if (msg.role === 'system') {
                systemInstructions.push(msg.content);
            } else if (msg.role === 'tool') {
                // Convert tool response
                contents.push({
                    role: 'user',
                    parts: [{
                        functionResponse: {
                            name: msg.name, // OpenAI requires name for tool role
                            response: typeof msg.content === 'string' ? { result: msg.content } : msg.content
                        }
                    }]
                });
            } else if (msg.role === 'user' || msg.role === 'assistant') {
                const role = msg.role === 'assistant' ? 'model' : 'user';
                const parts: any[] = [];

                // Handle Content
                if (msg.content) {
                    if (Array.isArray(msg.content)) {
                        for (const part of msg.content) {
                            if (part.type === 'text') parts.push({ text: part.text });
                            else if (part.type === 'image_url') {
                                const url = part.image_url.url;
                                if (url.startsWith('data:')) {
                                    const [meta, data] = url.split(',');
                                    const mimeType = meta.split(':')[1].split(';')[0];
                                    parts.push({ inlineData: { mimeType, data } });
                                }
                            }
                        }
                    } else {
                        parts.push({ text: msg.content });
                    }
                }

                // Handle Tool Calls (Assistant only)
                if (msg.tool_calls) {
                    for (const toolCall of msg.tool_calls) {
                        parts.push({
                            functionCall: {
                                name: toolCall.function.name,
                                args: typeof toolCall.function.arguments === 'string'
                                    ? JSON.parse(toolCall.function.arguments)
                                    : toolCall.function.arguments
                            }
                        });
                    }
                }

                if (parts.length > 0) {
                    contents.push({ role, parts });
                }
            }
        }

        // Default message if empty (Gemini requirement)
        if (contents.length === 0) {
            contents.push({ role: 'user', parts: [{ text: 'Hello' }] });
        }

        // 2. Generation Config
        const generationConfig: any = {
            topK: 64 // Default from gcli2api
        };
        if (openaiRequest.temperature != null) generationConfig.temperature = openaiRequest.temperature;
        if (openaiRequest.top_p != null) generationConfig.topP = openaiRequest.top_p;
        if (openaiRequest.max_tokens != null) generationConfig.maxOutputTokens = openaiRequest.max_tokens;
        if (openaiRequest.stop != null) generationConfig.stopSequences = Array.isArray(openaiRequest.stop) ? openaiRequest.stop : [openaiRequest.stop];

        // JSON Mode
        if (openaiRequest.response_format && openaiRequest.response_format.type === 'json_object') {
            generationConfig.responseMimeType = "application/json";
        }

        // Thinking (Gemini 3 thinking_level support, fallback to legacy heuristic)
        const modelName = String(openaiRequest.model || '').toLowerCase();
        const isGemini3Pro = modelName.includes('gemini-3-pro');
        const isGemini3Flash = modelName.includes('gemini-3-flash');
        const isImageModel = modelName.includes('image');
        const rawThinkingLevel = openaiRequest.thinking_level
            ?? openaiRequest.thinkingLevel
            ?? openaiRequest.thinkingConfig?.thinkingLevel
            ?? openaiRequest.thinkingConfig?.thinking_level
            ?? openaiRequest.generationConfig?.thinkingConfig?.thinkingLevel
            ?? openaiRequest.generationConfig?.thinkingConfig?.thinking_level
            ?? openaiRequest.generation_config?.thinking_config?.thinking_level
            ?? openaiRequest.generation_config?.thinking_config?.thinkingLevel;
        const derivedThinkingLevel = modelName.includes('gemini-3-pro-low')
            ? 'low'
            : modelName.includes('gemini-3-pro-high')
                ? 'high'
                : modelName.includes('gemini-3-flash-minimal')
                    ? 'minimal'
                    : modelName.includes('gemini-3-flash-medium')
                        ? 'medium'
                        : modelName.includes('gemini-3-flash-low')
                            ? 'low'
                            : modelName.includes('gemini-3-flash-high')
                                ? 'high'
                                : null;
        const candidateLevels: string[] = [];
        if (typeof rawThinkingLevel === 'string' && rawThinkingLevel.trim() !== '') {
            candidateLevels.push(rawThinkingLevel.trim().toLowerCase());
        }
        if (derivedThinkingLevel) candidateLevels.push(derivedThinkingLevel);
        if (!isImageModel && (isGemini3Pro || isGemini3Flash) && candidateLevels.length > 0) {
            const allowedLevels = isGemini3Flash
                ? ['minimal', 'medium', 'low', 'high']
                : ['low', 'high'];
            const selected = candidateLevels.find(level => allowedLevels.includes(level));
            if (selected) {
                generationConfig.thinkingConfig = { thinkingLevel: selected };
            }
        }
        if (!generationConfig.thinkingConfig && openaiRequest.model.includes('thinking')) {
            generationConfig.thinkingConfig = {
                includeThoughts: true,
                thinkingBudget: 1024
            };
        }

        // 3. Tools Definition
        if (openaiRequest.tools) {
            const transformedTools = transformTools(openaiRequest.tools);
            if (transformedTools.length > 0) {
                tools = transformedTools;
            }
        }

        // Google Search Tool
        if (openaiRequest.model.includes('search')) {
            // Only add if not already present (check structure)
            const hasSearch = tools.some(t => t.googleSearch);
            if (!hasSearch) {
                tools.push({ googleSearch: {} });
            }
        }

        // 4. Construct Payload
        const payload: any = {
            contents,
            generationConfig,
            safetySettings: mergeSafetySettings(openaiRequest.safety_settings || openaiRequest.safetySettings || [])
        };

        if (systemInstructions.length > 0) {
            payload.systemInstruction = { parts: [{ text: systemInstructions.join('\n\n') }] };
        }

        if (tools.length > 0) {
            payload.tools = tools;
        }

        // Tool Config (tool_choice)
        if (openaiRequest.tool_choice) {
            if (openaiRequest.tool_choice === 'auto') payload.toolConfig = { functionCallingConfig: { mode: 'AUTO' } };
            else if (openaiRequest.tool_choice === 'none') payload.toolConfig = { functionCallingConfig: { mode: 'NONE' } };
            else if (openaiRequest.tool_choice === 'required') payload.toolConfig = { functionCallingConfig: { mode: 'ANY' } };
            else if (typeof openaiRequest.tool_choice === 'object') {
                payload.toolConfig = {
                    functionCallingConfig: {
                        mode: 'ANY',
                        allowedFunctionNames: [openaiRequest.tool_choice.function.name]
                    }
                };
            }
        }

        return payload;
    }

    // --- Response Conversion Logic ---

    private static convertGeminiResponseToOpenAI(geminiResponse: any, model: string, usageMetadata?: any) {
        const choices = (geminiResponse.candidates || []).map((candidate: any) => {
            const parts = candidate.content?.parts || [];
            let content = '';
            let reasoning_content = '';
            const toolCalls: any[] = [];

            for (const part of parts) {
                if (part.functionCall) {
                    toolCalls.push({
                        id: 'call_' + crypto.randomUUID(),
                        type: 'function',
                        function: {
                            name: part.functionCall.name,
                            arguments: JSON.stringify(part.functionCall.args)
                        }
                    });
                } else if (part.text) {
                    if (part.thought) reasoning_content += part.text;
                    else content += part.text;
                }
            }

            const message: any = { role: 'assistant' };
            if (content) message.content = content;
            if (reasoning_content) message.reasoning_content = reasoning_content;
            if (toolCalls.length > 0) message.tool_calls = toolCalls;

            return {
                index: candidate.index || 0,
                message,
                finish_reason: candidate.finishReason === 'STOP' ? 'stop' : 'length'
            };
        });

        let usage = { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 };
        if (usageMetadata) {
            usage = {
                prompt_tokens: usageMetadata.promptTokenCount || 0,
                completion_tokens: usageMetadata.candidatesTokenCount || 0,
                total_tokens: usageMetadata.totalTokenCount || 0
            };
        }

        return {
            id: 'chatcmpl-' + crypto.randomUUID(),
            object: 'chat.completion',
            created: Math.floor(Date.now() / 1000),
            model,
            choices,
            usage
        };
    }

    private static convertGeminiChunkToOpenAI(geminiChunk: any, model: string, id: string, usageMetadata?: any) {
        const choices: any[] = [];
        const candidates = geminiChunk.candidates || [];

        for (const candidate of candidates) {
            const parts = candidate.content?.parts || [];
            let finishReason = null;

            if (candidate.finishReason === 'STOP') finishReason = 'stop';
            else if (candidate.finishReason === 'MAX_TOKENS') finishReason = 'length';
            else if (candidate.finishReason) finishReason = 'stop';

            // Extract text, reasoning, tools
            let content = '';
            let reasoning = '';
            const toolCalls: any[] = [];

            for (const part of parts) {
                if (part.functionCall) {
                    toolCalls.push({
                        index: 0,
                        id: 'call_' + crypto.randomUUID(), // Stream tool calls usually need distinct IDs
                        type: 'function',
                        function: {
                            name: part.functionCall.name,
                            arguments: JSON.stringify(part.functionCall.args)
                        }
                    });
                } else if (part.text) {
                    if (part.thought) reasoning += part.text;
                    else content += part.text;
                }
            }

            const delta: any = {};
            if (content) delta.content = content;
            if (reasoning) delta.reasoning_content = reasoning;
            if (toolCalls.length > 0) delta.tool_calls = toolCalls;

            choices.push({
                index: candidate.index || 0,
                delta,
                finish_reason: finishReason
            });
        }

        const chunk: any = {
            id,
            object: 'chat.completion.chunk',
            created: Math.floor(Date.now() / 1000),
            model,
            choices
        };

        if (usageMetadata) {
            chunk.usage = {
                prompt_tokens: usageMetadata.promptTokenCount || 0,
                completion_tokens: usageMetadata.candidatesTokenCount || 0,
                total_tokens: usageMetadata.totalTokenCount || 0
            };
        }

        return chunk;
    }

    // --- Helper: Parse Quota Reset Timestamp (Ported from utils.py) ---
    private static parseQuotaResetTimestamp(errorResponse: any): number | null {
        try {
            const error = errorResponse.error || {};
            const details = error.details || [];

            for (const detail of details) {
                if (detail['@type'] === 'type.googleapis.com/google.rpc.ErrorInfo') {
                    const metadata = detail.metadata || {};
                    let resetTimestampStr = metadata.quotaResetTimeStamp;

                    if (resetTimestampStr) {
                        if (resetTimestampStr.endsWith('Z')) {
                            resetTimestampStr = resetTimestampStr.replace('Z', '+00:00');
                        }
                        const resetDate = new Date(resetTimestampStr);
                        return resetDate.getTime();
                    }
                } else if (detail['@type'] === 'type.googleapis.com/google.rpc.RetryInfo') {
                    const retryDelayStr = detail.retryDelay;
                    if (retryDelayStr && retryDelayStr.endsWith('s')) {
                        const delaySeconds = parseFloat(retryDelayStr.slice(0, -1));
                        if (!isNaN(delaySeconds)) {
                            return Date.now() + (delaySeconds * 1000);
                        }
                    }
                }
            }
            return null;
        } catch (e) {
            return null;
        }
    }

    private static createErrorResponse(message: string, statusCode: number = 500): any {
        return { error: { message, type: 'api_error', code: statusCode } };
    }

    private static async recordSuccessfulCall(credentialId: number, modelName: string, userId: number) {
        const usage = resolveCliUsageGroup(modelName);
        const key = usage.statsKey;
        const globalKey = usage.group === 'flash'
            ? 'flash'
            : usage.group === 'pro'
                ? 'pro'
                : usage.group === 'v3'
                    ? 'v3'
                    : 'other';

        const todayStr = getTodayStrUTC8();
        const userStatsKey = `USER_STATS:${userId}:${todayStr}`;
        const globalStatsKey = `GLOBAL_STATS:${todayStr}`;

        try {
            // User Stats (Detailed keys)
            await redis.hincrby(userStatsKey, key, 1);
            await redis.expire(userStatsKey, 172800); // 2 days

            // Global Stats (Simplified keys for Admin Dashboard)
            if (globalKey !== 'other') {
                await redis.hincrby(globalStatsKey, globalKey, 1);
                await redis.expire(globalStatsKey, 172800);
            }
        } catch (e) { }
    }

    // --- Core Request Execution (Ported from google_chat_api.py: send_gemini_request) ---
    private static async sendGeminiRequest(
        modelName: string,
        payload: any,
        isStreaming: boolean,
        credentialId: number,
        accessToken: string,
        projectId: string,
        onStreamChunk?: (chunkStr: string) => Promise<void>
    ): Promise<any> {
        const MAX_RETRIES = 5; // From gcli2api config
        const RETRY_INTERVAL = 1000; // 1 second, from gcli2api config

        const baseUrl = process.env.GOOGLE_CLOUD_CODE_URL || 'https://cloudcode-pa.googleapis.com';
        const action = isStreaming ? 'streamGenerateContent' : 'generateContent';
        let endpoint = `${baseUrl}/v1internal:${action}`;
        if (isStreaming) {
            endpoint += '?alt=sse';
        }

        const finalPayload = {
            model: resolveCloudCodeModelName(modelName),
            project: projectId,
            request: payload
        };

        const headers = {
            'Authorization': `Bearer ${accessToken}`,
            'Content-Type': 'application/json',
            'User-Agent': getUserAgent()
        };

        for (let attempt = 0; attempt < MAX_RETRIES + 1; attempt++) {
            try {
                if (isStreaming) {
                    return await new Promise<void>((resolve, reject) => {
                        const bufferLine = { val: '' };

                        const transformer = new Transform({
                            writableObjectMode: true,
                            transform(chunk, encoding, callback) {
                                bufferLine.val += chunk.toString();
                                const lines = bufferLine.val.split('\n');
                                bufferLine.val = lines.pop() || '';
                                (async () => {
                                    for (const line of lines) {
                                        if (line.trim()) await onStreamChunk!(line);
                                    }
                                    callback();
                                })();
                            },
                            flush(callback) {
                                if (bufferLine.val.trim()) {
                                    (async () => {
                                        await onStreamChunk!(bufferLine.val);
                                        callback();
                                    })();
                                } else {
                                    callback();
                                }
                            }
                        });

                        stream(endpoint, {
                            method: 'POST',
                            headers,
                            body: JSON.stringify(finalPayload),
                            opaque: { reject, credentialId }
                        }, ({ statusCode, opaque }: any) => {
                            const { reject, credentialId } = opaque;

                            if (statusCode !== 200) {
                                let errBody = '';
                                const errStream = new PassThrough();
                                errStream.setEncoding('utf8');
                                errStream.on('data', c => errBody += c);
                                errStream.on('end', async () => {
                                    if (statusCode === 429) {
                                        try {
                                            const errJson = JSON.parse(errBody);
                                            const cooldown = ProxyController.parseQuotaResetTimestamp(errJson);
                                            if (cooldown !== null) {
                                                await poolManager.markAsCooling(credentialId, cooldown);
                                            } else {
                                                await poolManager.markAsCooling(credentialId);
                                            }
                                        } catch (e) {
                                            await poolManager.markAsCooling(credentialId);
                                        }
                                    } else if (statusCode === 403) {
                                        await poolManager.markAsDead(credentialId);
                                    }
                                    reject(makeHttpError(statusCode, errBody));
                                });
                                return errStream;
                            }

                            return transformer;
                        }).then(() => resolve()).catch(reject);
                    });
                } else {
                    const { statusCode, body } = await undiciRequest(endpoint, {
                        method: 'POST',
                        headers,
                        body: JSON.stringify(finalPayload)
                    });

                    if (statusCode !== 200) {
                        const errText = await body.text();
                        if (statusCode === 429) {
                            try {
                                const errJson = JSON.parse(errText);
                                const cooldown = ProxyController.parseQuotaResetTimestamp(errJson);
                                if (cooldown !== null) {
                                    await poolManager.markAsCooling(credentialId, cooldown);
                                } else {
                                    await poolManager.markAsCooling(credentialId);
                                }
                            } catch (e) {
                                await poolManager.markAsCooling(credentialId);
                            }
                        } else if (statusCode === 403) {
                            await poolManager.markAsDead(credentialId);
                        }
                        throw makeHttpError(statusCode, errText);
                    }

                    const rawData = await body.json() as any;
                    // Google API returns { response: { ... }, usageMetadata: { ... } }
                    // We need to merge them or just return the whole thing and handle extraction later.
                    // Let's return the whole thing to be safe and consistent with streaming logic potentially.
                    return rawData;
                }
            } catch (e: any) {
                console.warn(`[Proxy] Attempt ${attempt + 1}/${MAX_RETRIES + 1} failed: ${e.message}`);
                if (attempt < MAX_RETRIES) {
                    await new Promise(r => setTimeout(r, RETRY_INTERVAL));
                } else {
                    throw e; // Max retries reached
                }
            }
        }
        throw new Error('Max retries exceeded and failed to get a response.');
    }

    // --- Strategy Handlers (Ported from openai_router.py) ---
    private static async handleStandardRequest(req: FastifyRequest, reply: FastifyReply, modelName: string, geminiPayload: any, user: any, isAdminKey: boolean, poolType: 'GLOBAL' | 'V3' = 'GLOBAL') {
        const MAX_429_RETRIES = 3;

        // ç«‹å³è®¡æ•°ï¼ˆä¸ç­‰å¾… API å“åº”ï¼‰
        if (!isAdminKey) {
            await prisma.user.update({ where: { id: user.id }, data: { today_used: { increment: 1 } } }).catch(() => { });
        }
        let lastErr: any = null;
        for (let attempt = 0; attempt < MAX_429_RETRIES; attempt++) {
            const cred = await poolManager.getRoundRobinCredential(poolType);
            if (!cred) {
                return reply.code(500).send(ProxyController.createErrorResponse('No valid credentials available', 500));
            }
            try {
                const googleResponse = await ProxyController.sendGeminiRequest(
                    modelName, geminiPayload, false, cred.credentialId, cred.accessToken, cred.projectId
                );
                const geminiResponse = googleResponse.response || googleResponse;
                const usageMetadata = googleResponse.usageMetadata;
                const openaiResponse = ProxyController.convertGeminiResponseToOpenAI(geminiResponse, modelName, usageMetadata);
                await ProxyController.recordSuccessfulCall(cred.credentialId, modelName, user.id);
                return reply.send(openaiResponse);
            } catch (error: any) {
                if (isHttpError(error) && error.statusCode === 429) {
                    lastErr = error;
                    continue;
                }
                console.error('[Proxy] Standard request error:', error);
                return reply.code(500).send(ProxyController.createErrorResponse(error.message, 500));
            }
        }
        if (lastErr) {
            return reply.code(429).send(ProxyController.createErrorResponse(lastErr.body || lastErr.message, 429));
        }
    }

    private static async handleStreamRequest(req: FastifyRequest, reply: FastifyReply, modelName: string, geminiPayload: any, user: any, isAdminKey: boolean, poolType: 'GLOBAL' | 'V3' = 'GLOBAL') {
        const MAX_429_RETRIES = 3;

        reply.raw.writeHead(200, {
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'X-Accel-Buffering': 'no'
        });

        const responseId = 'chatcmpl-' + crypto.randomUUID();

        // ç«‹å³è®¡æ•°ï¼ˆä¸ç­‰å¾…é¦–ä¸ª chunkï¼‰
        if (!isAdminKey) {
            await prisma.user.update({ where: { id: user.id }, data: { today_used: { increment: 1 } } }).catch(() => { });
        }
        let lastErr: any = null;
        for (let attempt = 0; attempt < MAX_429_RETRIES; attempt++) {
            const cred = await poolManager.getRoundRobinCredential(poolType);
            if (!cred) {
                reply.raw.writeHead(500, { 'Content-Type': 'application/json' });
                reply.raw.end(JSON.stringify(ProxyController.createErrorResponse('No valid credentials available', 500)));
                return;
            }
            try {
                await ProxyController.sendGeminiRequest(
                    modelName, geminiPayload, true, cred.credentialId, cred.accessToken, cred.projectId,
                    async (chunkStr) => {
                        if (chunkStr.startsWith('data: ')) {
                            const jsonStr = chunkStr.substring(6);
                            try {
                                const geminiChunk = JSON.parse(jsonStr);
                                const data = geminiChunk.response || geminiChunk;
                                const usageMetadata = geminiChunk.usageMetadata;
                                const openaiChunk = ProxyController.convertGeminiChunkToOpenAI(data, modelName, responseId, usageMetadata);
                                reply.raw.write(`data: ${JSON.stringify(openaiChunk)}\n\n`);
                            } catch (e) {
                                console.error('Error parsing/converting stream chunk:', e);
                            }
                        }
                    }
                );
                await ProxyController.recordSuccessfulCall(cred.credentialId, modelName, user.id);
                reply.raw.write('data: [DONE]\n\n');
                reply.raw.end();
                return;
            } catch (error: any) {
                if (isHttpError(error) && error.statusCode === 429) {
                    lastErr = error;
                    continue;
                }
                console.error('[Proxy] Stream request error:', error);
                const errPayload = ProxyController.createErrorResponse(error.message, 500);
                reply.raw.write(`data: ${JSON.stringify(errPayload)}\n\n`);
                reply.raw.write('data: [DONE]\n\n');
                reply.raw.end();
                return;
            }
        }
        if (lastErr) {
            const errPayload = ProxyController.createErrorResponse(lastErr.body || lastErr.message, 429);
            reply.raw.write(`data: ${JSON.stringify(errPayload)}\n\n`);
        }
        reply.raw.write('data: [DONE]\n\n');
        reply.raw.end();
    }

    private static async handleFakeStreamRequest(req: FastifyRequest, reply: FastifyReply, modelName: string, geminiPayload: any, user: any, isAdminKey: boolean, poolType: 'GLOBAL' | 'V3' = 'GLOBAL') {
        const MAX_429_RETRIES = 3;

        reply.raw.writeHead(200, {
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'X-Accel-Buffering': 'no'
        });

        const responseId = 'chatcmpl-' + crypto.randomUUID();
        const created = Math.floor(Date.now() / 1000);

        // Heartbeat: Keep connection alive while waiting for generation
        const heartbeatInterval = setInterval(() => {
            const heartbeat = {
                id: responseId,
                object: "chat.completion.chunk",
                created,
                model: modelName,
                choices: [{ index: 0, delta: { role: 'assistant', content: '' }, finish_reason: null }]
            };
            reply.raw.write(`data: ${JSON.stringify(heartbeat)}\n\n`);
        }, 2000); // Slower heartbeat to reduce noise

        if (!isAdminKey) {
            await prisma.user.update({ where: { id: user.id }, data: { today_used: { increment: 1 } } }).catch(() => { });
        }
        let lastErr: any = null;
        for (let attempt = 0; attempt < MAX_429_RETRIES; attempt++) {
            const cred = await poolManager.getRoundRobinCredential(poolType);
            if (!cred) {
                reply.raw.writeHead(500, { 'Content-Type': 'application/json' });
                reply.raw.end(JSON.stringify(ProxyController.createErrorResponse('No valid credentials available', 500)));
                clearInterval(heartbeatInterval);
                return;
            }
            try {
                const geminiResponse = await ProxyController.sendGeminiRequest(
                    modelName, geminiPayload, false, cred.credentialId, cred.accessToken, cred.projectId
                );
                clearInterval(heartbeatInterval);
                await ProxyController.recordSuccessfulCall(cred.credentialId, modelName, user.id);

                // Extract content (Fix: Handle .response wrapper and Safety)
                const data = geminiResponse.response || geminiResponse;
                const candidates = data.candidates || [];
                const candidate = candidates[0] || {};
                const parts = candidate.content?.parts || [];

                let content = '';
                let reasoning = '';

                for (const part of parts) {
                    if (part.text) {
                        if (part.thought) reasoning += part.text;
                        else content += part.text;
                    }
                }

                if (!content && !reasoning) {
                    content = '';
                    if (candidate.finishReason === 'SAFETY') {
                        content = 'ğŸš« [è¯¥å›å¤å› å®‰å…¨ç­–ç•¥è¢«æ‹¦æˆª / Content blocked by safety filters]';
                    } else if (candidate.finishReason === 'RECITATION') {
                        content = 'ğŸš« [è¯¥å›å¤å› ç‰ˆæƒ/å¼•ç”¨åŸå› è¢«æ‹¦æˆª / Content blocked by recitation check]';
                    } else if (!reasoning) {
                        content = '[No text content returned from model. Raw status: ' + (candidate.finishReason || 'UNKNOWN') + ']';
                    }
                }

                // Helper: Send full content at once
                const sendFullChunk = (text: string, isReasoning: boolean) => {
                    if (!text) return;

                    const delta: any = {};
                    if (isReasoning) delta.reasoning_content = text;
                    else delta.content = text;

                    const chunk = {
                        id: responseId, object: "chat.completion.chunk", created, model: modelName,
                        choices: [{ index: 0, delta, finish_reason: null }]
                    };
                    reply.raw.write(`data: ${JSON.stringify(chunk)}\n\n`);
                };

                // 1. Send Reasoning (if any)
                if (reasoning) {
                    sendFullChunk(reasoning, true);
                }

                // 2. Send Content (if any)
                if (content) {
                    sendFullChunk(content, false);
                }

                // End chunk
                const usageMetadata = geminiResponse.usageMetadata;
                let usage = undefined;
                if (usageMetadata) {
                    usage = {
                        prompt_tokens: usageMetadata.promptTokenCount || 0,
                        completion_tokens: usageMetadata.candidatesTokenCount || 0,
                        total_tokens: usageMetadata.totalTokenCount || 0
                    };
                }

                const endChunk: any = {
                    id: responseId,
                    object: "chat.completion.chunk",
                    created,
                    model: modelName,
                    choices: [{ index: 0, delta: {}, finish_reason: candidate.finishReason === 'STOP' ? 'stop' : 'length' }]
                };
                if (usage) endChunk.usage = usage;

                reply.raw.write(`data: ${JSON.stringify(endChunk)}\n\n`);

                reply.raw.write('data: [DONE]\n\n');
                reply.raw.end();
                return;
            } catch (error: any) {
                if (isHttpError(error) && error.statusCode === 429) {
                    lastErr = error;
                    continue;
                }
                clearInterval(heartbeatInterval);
                console.error('[Proxy] Fake stream request error:', error);
                const errPayload = ProxyController.createErrorResponse(error.message, 500);
                reply.raw.write(`data: ${JSON.stringify(errPayload)}\n\n`);
                reply.raw.write('data: [DONE]\n\n');
                reply.raw.end();
                return;
            }
        }
        clearInterval(heartbeatInterval);
        if (lastErr) {
            const errPayload = ProxyController.createErrorResponse(lastErr.body || lastErr.message, 429);
            reply.raw.write(`data: ${JSON.stringify(errPayload)}\n\n`);
        }
        reply.raw.write('data: [DONE]\n\n');
        reply.raw.end();
    }

    private static async handleAntiTruncationStream(req: FastifyRequest, reply: FastifyReply, modelName: string, geminiPayload: any, user: any, isAdminKey: boolean, poolType: 'GLOBAL' | 'V3' = 'GLOBAL') {
        // Ported from gcli2api/anti_truncation.py#apply_anti_truncation_to_stream
        // This is complex and involves recursive calls to sendGeminiRequest for "Continue" prompts.
        // For now, it will be a simplified pass-through to sendGeminiRequest with streaming.
        // Full anti-truncation logic will require more complex state management and message history.
        console.warn("Anti-truncation stream is not fully implemented yet, falling back to standard streaming.");
        await ProxyController.handleStreamRequest(req, reply, modelName, geminiPayload, user, isAdminKey, poolType);
    }

    // ============================================================
    // å¤šæ ¼å¼ API å…¼å®¹æ–¹æ³•
    // ============================================================

    /**
     * å¤„ç† Gemini åŸç”Ÿæ ¼å¼è¯·æ±‚ (generateContent)
     * è·¯ç”±: POST /v1/models/:model/generateContent
     */
    static async handleGeminiNative(req: FastifyRequest, reply: FastifyReply) {
        const { model } = req.params as { model: string };
        const body = req.body as any;

        // åˆå¹¶æ¨¡å‹åˆ°è¯·æ±‚ä½“
        const geminiBody = { ...body, model };

        // è½¬æ¢ä¸º OpenAI æ ¼å¼å¤„ç†
        const { request: openaiRequest } = normalizeToOpenAI(geminiBody, model);

        // è®¾ç½®éæµå¼
        openaiRequest.stream = false;

        // åˆ›å»ºæ¨¡æ‹Ÿè¯·æ±‚å¯¹è±¡
        const modifiedReq = {
            ...req,
            body: openaiRequest
        } as FastifyRequest;

        // ä½¿ç”¨åŒ…è£…å™¨å¤„ç†å¹¶è½¬æ¢å“åº”
        const originalSend = reply.send.bind(reply);
        reply.send = (payload: any) => {
            // è½¬æ¢å“åº”ä¸º Gemini æ ¼å¼
            const geminiResponse = openaiResponseToGemini(payload);
            return originalSend(geminiResponse);
        };

        await ProxyController.handleChatCompletion(modifiedReq, reply);
    }

    /**
     * å¤„ç† Gemini åŸç”Ÿæ ¼å¼æµå¼è¯·æ±‚ (streamGenerateContent)
     * è·¯ç”±: POST /v1/models/:model/streamGenerateContent
     */
    static async handleGeminiNativeStream(req: FastifyRequest, reply: FastifyReply) {
        const { model } = req.params as { model: string };
        const body = req.body as any;

        // åˆå¹¶æ¨¡å‹åˆ°è¯·æ±‚ä½“å¹¶è®¾ç½®æµå¼
        const geminiBody = { ...body, model, stream: true };

        // è½¬æ¢ä¸º OpenAI æ ¼å¼
        const { request: openaiRequest } = normalizeToOpenAI(geminiBody, model);
        openaiRequest.stream = true;

        // åˆ›å»ºæ¨¡æ‹Ÿè¯·æ±‚
        const modifiedReq = {
            ...req,
            body: openaiRequest
        } as FastifyRequest;

        // å¯¹äºæµå¼å“åº”ï¼Œæˆ‘ä»¬éœ€è¦åŒ…è£…åŸå§‹å“åº”æµ
        // ç”±äº handleChatCompletion ç›´æ¥å†™å…¥ reply.rawï¼Œè¿™é‡Œéœ€è¦ç‰¹æ®Šå¤„ç†
        // æš‚æ—¶ä½¿ç”¨ç®€åŒ–æ–¹æ¡ˆï¼šä½¿ç”¨ OpenAI æ ¼å¼è¿”å›
        // TODO: å®ç°å®Œæ•´çš„æµå¼æ ¼å¼è½¬æ¢
        console.log('[MultiFormat] Gemini streaming request, model:', model);
        await ProxyController.handleChatCompletion(modifiedReq, reply);
    }

    /**
     * å¤„ç† Anthropic åŸç”Ÿæ ¼å¼è¯·æ±‚ (messages)
     * è·¯ç”±: POST /v1/messages
     */
    static async handleAnthropicNative(req: FastifyRequest, reply: FastifyReply) {
        const body = req.body as any;
        const originalModel = body.model || 'claude-sonnet-4-5';
        const isStreaming = body.stream === true;

        // è½¬æ¢ä¸º OpenAI æ ¼å¼
        const { request: openaiRequest } = normalizeToOpenAI(body);

        // åˆ›å»ºæ¨¡æ‹Ÿè¯·æ±‚
        const modifiedReq = {
            ...req,
            body: openaiRequest
        } as FastifyRequest;

        if (!isStreaming) {
            // éæµå¼ï¼šåŒ…è£…å“åº”
            const originalSend = reply.send.bind(reply);
            reply.send = (payload: any) => {
                const anthropicResponse = openaiResponseToAnthropic(payload, originalModel);
                return originalSend(anthropicResponse);
            };
        }
        // å¯¹äºæµå¼ï¼Œæš‚æ—¶ä½¿ç”¨ OpenAI SSE æ ¼å¼
        // TODO: å®ç° Anthropic SSE æ ¼å¼è½¬æ¢

        console.log('[MultiFormat] Anthropic request, model:', originalModel, 'streaming:', isStreaming);
        await ProxyController.handleChatCompletion(modifiedReq, reply);
    }

    /**
     * é€šç”¨å…¥å£ï¼šè‡ªåŠ¨æ£€æµ‹è¯·æ±‚æ ¼å¼å¹¶å¤„ç†
     * å¯ç”¨äº /v1/chat/completions è‡ªåŠ¨å…¼å®¹æ‰€æœ‰æ ¼å¼
     */
    static async handleUniversalRequest(req: FastifyRequest, reply: FastifyReply) {
        const body = req.body as any;
        const format = detectRequestFormat(body);

        console.log('[MultiFormat] Auto-detected format:', format);

        if (format === 'gemini') {
            // æ£€æŸ¥æ˜¯å¦æµå¼
            if (body.stream === true) {
                return ProxyController.handleGeminiNativeStream(req, reply);
            }
            return ProxyController.handleGeminiNative(req, reply);
        } else if (format === 'anthropic') {
            return ProxyController.handleAnthropicNative(req, reply);
        } else {
            // OpenAI æ ¼å¼ï¼Œç›´æ¥å¤„ç†
            return ProxyController.handleChatCompletion(req, reply);
        }
    }
}
